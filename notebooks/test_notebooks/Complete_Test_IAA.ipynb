{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Test all the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First only the corpus ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import collections \n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from itertools import combinations\n",
    "sys.path.insert(0, '../..')\n",
    "from src.experiment_utils.helper_classes import token, span, repository\n",
    "from src.d02_corpus_statistics.corpus import Corpus\n",
    "from src.d03_inter_annotator_agreement.inter_annotator_agremment import Inter_Annotator_Agreement, _get_score_article, _get_curation_annotator_score\n",
    "from definitions import df_annotation_marker\n",
    "from src.d03_inter_annotator_agreement.inter_annotator_agremment import row_to_span_list, keep_valid_anotations\n",
    "from src.d03_inter_annotator_agreement.scoring_functions import create_scoring_matrix\n",
    "from src.d03_inter_annotator_agreement.scoring_functions import f1_exact\n",
    "\n",
    "from definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  improve_scores\u001b[m\n",
      "  master\u001b[m\n",
      "  plots\u001b[m\n",
      "* \u001b[32mupdate_pygamma\u001b[m\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "!git branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test all corpus functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "#dataframe_dir = os.path.join(ROOT_DIR,'data/02_processed_to_dataframe', 'preprocessed_dataframe.pkl')\n",
    "dataframe_dir = '/home/jkuettel/NLP_spark/data/02_processed_to_dataframe/preprocessed_dataframe_test.pkl'\n",
    "\n",
    "stat_df = pd.read_pickle(dataframe_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Article_State</th>\n",
       "      <th>Finished_Annotators</th>\n",
       "      <th>Curation</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EU_32018R1999_Title_0_Chapter_7_Section_3_Article_43</th>\n",
       "      <td></td>\n",
       "      <td>article 43\\r\\nexercise of the delegation\\r\\n1....</td>\n",
       "      <td>[start:0 stop:7 text:article tag_count:0, star...</td>\n",
       "      <td>CURATION_FINISHED</td>\n",
       "      <td>[A, B]</td>\n",
       "      <td>[span id:CUR0 annotator:Curation layer:Instrum...</td>\n",
       "      <td>[span id:A1 annotator:A layer:Instrumenttypes ...</td>\n",
       "      <td>[span id:B1 annotator:B layer:Policydesignchar...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_32019R0631_Title_0_Chapter_0_Section_0_Article_12</th>\n",
       "      <td></td>\n",
       "      <td>article 12\\r\\nreal-world co2 emissions and fue...</td>\n",
       "      <td>[start:0 stop:7 text:article tag_count:0, star...</td>\n",
       "      <td>CURATION_FINISHED</td>\n",
       "      <td>[C, D]</td>\n",
       "      <td>[span id:CUR36 annotator:Curation layer:Instru...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[span id:C1 annotator:C layer:Instrumenttypes ...</td>\n",
       "      <td>[span id:D1 annotator:D layer:Policydesignchar...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_32018L2001_Title_0_Chapter_0_Section_0_Article_11</th>\n",
       "      <td></td>\n",
       "      <td>article 11\\r\\njoint projects between member st...</td>\n",
       "      <td>[start:0 stop:7 text:article tag_count:0, star...</td>\n",
       "      <td>CURATION_FINISHED</td>\n",
       "      <td>[B, C]</td>\n",
       "      <td>[span id:CUR116 annotator:Curation layer:Instr...</td>\n",
       "      <td></td>\n",
       "      <td>[span id:B28 annotator:B layer:Instrumenttypes...</td>\n",
       "      <td>[span id:C58 annotator:C layer:Instrumenttypes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_32018R1999_Title_0_Chapter_7_Section_3_Article_56</th>\n",
       "      <td></td>\n",
       "      <td>article 56\\r\\namendments to directive (eu) 201...</td>\n",
       "      <td>[start:0 stop:7 text:article tag_count:0, star...</td>\n",
       "      <td>CURATION_FINISHED</td>\n",
       "      <td>[A, B]</td>\n",
       "      <td>[span id:CUR202 annotator:Curation layer:Polic...</td>\n",
       "      <td>[span id:A38 annotator:A layer:Policydesigncha...</td>\n",
       "      <td>[span id:B129 annotator:B layer:Policydesignch...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_32018L2001_Title_0_Chapter_0_Section_0_Article_03</th>\n",
       "      <td></td>\n",
       "      <td>article 3\\r\\nbinding overall union target for ...</td>\n",
       "      <td>[start:0 stop:7 text:article tag_count:0, star...</td>\n",
       "      <td>CURATION_FINISHED</td>\n",
       "      <td>[B, C, D]</td>\n",
       "      <td>[span id:CUR211 annotator:Curation layer:Instr...</td>\n",
       "      <td></td>\n",
       "      <td>[span id:B138 annotator:B layer:Instrumenttype...</td>\n",
       "      <td>[span id:C165 annotator:C layer:Instrumenttype...</td>\n",
       "      <td>[span id:D27 annotator:D layer:Instrumenttypes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Policy  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...          \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...          \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...          \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...          \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...          \n",
       "\n",
       "                                                                                                 Text  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  article 43\\r\\nexercise of the delegation\\r\\n1....   \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...  article 12\\r\\nreal-world co2 emissions and fue...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  article 11\\r\\njoint projects between member st...   \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  article 56\\r\\namendments to directive (eu) 201...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  article 3\\r\\nbinding overall union target for ...   \n",
       "\n",
       "                                                                                               Tokens  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  [start:0 stop:7 text:article tag_count:0, star...   \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...  [start:0 stop:7 text:article tag_count:0, star...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [start:0 stop:7 text:article tag_count:0, star...   \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  [start:0 stop:7 text:article tag_count:0, star...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [start:0 stop:7 text:article tag_count:0, star...   \n",
       "\n",
       "                                                        Article_State  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  CURATION_FINISHED   \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...  CURATION_FINISHED   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  CURATION_FINISHED   \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  CURATION_FINISHED   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  CURATION_FINISHED   \n",
       "\n",
       "                                                   Finished_Annotators  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...              [A, B]   \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...              [C, D]   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...              [B, C]   \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...              [A, B]   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...           [B, C, D]   \n",
       "\n",
       "                                                                                             Curation  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  [span id:CUR0 annotator:Curation layer:Instrum...   \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...  [span id:CUR36 annotator:Curation layer:Instru...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [span id:CUR116 annotator:Curation layer:Instr...   \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  [span id:CUR202 annotator:Curation layer:Polic...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [span id:CUR211 annotator:Curation layer:Instr...   \n",
       "\n",
       "                                                                                                    A  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  [span id:A1 annotator:A layer:Instrumenttypes ...   \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...                                                      \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...                                                      \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  [span id:A38 annotator:A layer:Policydesigncha...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...                                                      \n",
       "\n",
       "                                                                                                    B  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  [span id:B1 annotator:B layer:Policydesignchar...   \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...                                                      \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [span id:B28 annotator:B layer:Instrumenttypes...   \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...  [span id:B129 annotator:B layer:Policydesignch...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [span id:B138 annotator:B layer:Instrumenttype...   \n",
       "\n",
       "                                                                                                    C  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...                                                      \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...  [span id:C1 annotator:C layer:Instrumenttypes ...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [span id:C58 annotator:C layer:Instrumenttypes...   \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...                                                      \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [span id:C165 annotator:C layer:Instrumenttype...   \n",
       "\n",
       "                                                                                                    D  \\\n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...                                                      \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...  [span id:D1 annotator:D layer:Policydesignchar...   \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...                                                      \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...                                                      \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...  [span id:D27 annotator:D layer:Instrumenttypes...   \n",
       "\n",
       "                                                   E F G  \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...        \n",
       "EU_32019R0631_Title_0_Chapter_0_Section_0_Artic...        \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...        \n",
       "EU_32018R1999_Title_0_Chapter_7_Section_3_Artic...        \n",
       "EU_32018L2001_Title_0_Chapter_0_Section_0_Artic...        "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412, 13)\n"
     ]
    }
   ],
   "source": [
    "test_evaluator = Inter_Annotator_Agreement(stat_df, front_and_whereas = False, DEBUG = DEBUG)\n",
    "test_evaluator.keep_only_finished_articles()\n",
    "print(test_evaluator.df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact_pygamma', 'f1_tokenwise_pygamma', 'f1_partial_pygamma', 'f1_heuristic', 'f1_exact', 'f1_tokenwise', 'f1_positional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/412 [00:00<00:01, 391.81it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CombinedCategoricalDissimilarity.__init__() got an unexpected keyword argument 'categories'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_evaluator\u001b[39m.\u001b[39;49mappend_total_score_per_article(scoring_metrics)\n\u001b[1;32m      2\u001b[0m test_evaluator\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/NLP_spark/notebooks/test_notebooks/../../src/d03_inter_annotator_agreement/inter_annotator_agremment.py:252\u001b[0m, in \u001b[0;36mInter_Annotator_Agreement.append_total_score_per_article\u001b[0;34m(self, scoring_metrics, parallel, **optional_tuple_properties)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[39mif\u001b[39;00m column_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mcolumns:\n\u001b[1;32m    250\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[column_name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49mprogress_apply(\u001b[39mlambda\u001b[39;49;00m row: _get_score_article(row_to_span_list(row), scoring_metric, row[\u001b[39m'\u001b[39;49m\u001b[39mFinished_Annotators\u001b[39;49m\u001b[39m'\u001b[39;49m],  \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptional_tuple_properties), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculated_iaa_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculated_iaa_scores \u001b[39m+\u001b[39m scoring_metrics_to_calc\n",
      "File \u001b[0;32m~/miniconda3/envs/polianna/lib/python3.10/site-packages/tqdm/std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(df, df_function)(wrapper, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    815\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     t\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/polianna/lib/python3.10/site-packages/pandas/core/frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9554\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9556\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9557\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9558\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9563\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9564\u001b[0m )\n\u001b[0;32m-> 9565\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/polianna/lib/python3.10/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/envs/polianna/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/miniconda3/envs/polianna/lib/python3.10/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/polianna/lib/python3.10/site-packages/tqdm/std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    804\u001b[0m     \u001b[39m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[39m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     t\u001b[39m.\u001b[39mupdate(n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mtotal \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 809\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/NLP_spark/notebooks/test_notebooks/../../src/d03_inter_annotator_agreement/inter_annotator_agremment.py:252\u001b[0m, in \u001b[0;36mInter_Annotator_Agreement.append_total_score_per_article.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[39mif\u001b[39;00m column_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mcolumns:\n\u001b[1;32m    250\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[column_name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m row: _get_score_article(row_to_span_list(row), scoring_metric, row[\u001b[39m'\u001b[39;49m\u001b[39mFinished_Annotators\u001b[39;49m\u001b[39m'\u001b[39;49m],  \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptional_tuple_properties), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculated_iaa_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculated_iaa_scores \u001b[39m+\u001b[39m scoring_metrics_to_calc\n",
      "File \u001b[0;32m~/NLP_spark/notebooks/test_notebooks/../../src/d03_inter_annotator_agreement/inter_annotator_agremment.py:154\u001b[0m, in \u001b[0;36m_get_score_article\u001b[0;34m(span_list, scoring_metric, finished_annotators, **optional_tuple_properties)\u001b[0m\n\u001b[1;32m    151\u001b[0m         score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(spans_a1) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(spans_a2))\n\u001b[1;32m    153\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m         score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m scoring_metrics[scoring_metric] (span_list_annotator_pair, annotator_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptional_tuple_properties)\n\u001b[1;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m score\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(combinations(annotators,\u001b[39m2\u001b[39m)))\n",
      "File \u001b[0;32m~/NLP_spark/notebooks/test_notebooks/../../src/d03_inter_annotator_agreement/scoring_functions.py:130\u001b[0m, in \u001b[0;36mf1_exact_pygamma\u001b[0;34m(span_list_annotator_pair, annotator_pair, **optional_tuple_properties)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_exact_pygamma\u001b[39m(span_list_annotator_pair, annotator_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptional_tuple_properties):\n\u001b[0;32m--> 130\u001b[0m     tuple_list \u001b[39m=\u001b[39m create_tuples_pygamma(span_list_annotator_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptional_tuple_properties)\n\u001b[1;32m    131\u001b[0m     \u001b[39m# for a given tuple, treat tuple[0] as prediciton and tuple[1] as gold standart since the score is symmetric\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39m# note that all the spans of a certain annotator that arre missing in the counterpart are matched to a \"None\" Tag\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     exact \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([n_tuple[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mexact_match(n_tuple[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m n_tuple \u001b[39min\u001b[39;00m tuple_list \u001b[39mif\u001b[39;00m n_tuple[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtag_ \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m n_tuple[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtag_ \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m])\n",
      "File \u001b[0;32m~/NLP_spark/notebooks/test_notebooks/../../src/d03_inter_annotator_agreement/span_matching.py:11\u001b[0m, in \u001b[0;36mcreate_tuples_pygamma\u001b[0;34m(span_list, **dissimilarity_properties)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m spanlist_span \u001b[39min\u001b[39;00m span_list:\n\u001b[1;32m     10\u001b[0m     continuum\u001b[39m.\u001b[39madd(spanlist_span\u001b[39m.\u001b[39mannotator, Segment(spanlist_span\u001b[39m.\u001b[39mstart, spanlist_span\u001b[39m.\u001b[39mstop), spanlist_span\u001b[39m.\u001b[39mtag_)\n\u001b[0;32m---> 11\u001b[0m dissim \u001b[39m=\u001b[39m CombinedCategoricalDissimilarity(categories \u001b[39m=\u001b[39;49m dissimilarity_properties\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mcategory_list_matching\u001b[39;49m\u001b[39m'\u001b[39;49m,continuum\u001b[39m.\u001b[39;49mcategories), alpha\u001b[39m=\u001b[39;49mdissimilarity_properties\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39malpha\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1\u001b[39;49m),   beta\u001b[39m=\u001b[39;49mdissimilarity_properties\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mbeta\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1\u001b[39;49m), cat_dissimilarity_matrix \u001b[39m=\u001b[39;49m dissimilarity_properties\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mcat_dissimilarity_matrix_matching\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m     12\u001b[0m best_alignment \u001b[39m=\u001b[39m continuum\u001b[39m.\u001b[39mget_best_alignment(dissim)\n\u001b[1;32m     14\u001b[0m \u001b[39m#now retrieve spantuples\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: CombinedCategoricalDissimilarity.__init__() got an unexpected keyword argument 'categories'"
     ]
    }
   ],
   "source": [
    "\n",
    "test_evaluator.append_total_score_per_article(scoring_metrics)\n",
    "test_evaluator.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_exact_pygamma_score': 0.40028724799007476,\n",
       " 'f1_tokenwise_pygamma_score': 0.397687590370067,\n",
       " 'f1_partial_pygamma_score': 0.5159440267362017,\n",
       " 'f1_heuristic_score': 0.5275951402554365,\n",
       " 'f1_exact_score': 0.40028724799007476,\n",
       " 'f1_tokenwise_score': 0.5210605573521196,\n",
       " 'f1_positional_score': 0.7168747609545078}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_exact_pygamma_score': 0.40028724799007476,\n",
       " 'f1_tokenwise_pygamma_score': 0.397687590370067,\n",
       " 'f1_partial_pygamma_score': 0.5159440267362017,\n",
       " 'f1_heuristic_score': 0.5275951402554365,\n",
       " 'f1_exact_score': 0.40028724799007476,\n",
       " 'f1_tokenwise_score': 0.5210605573521196,\n",
       " 'f1_positional_score': 0.7168747609545078}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5210605573521196"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5439488370573863 + 0.4981722776468529)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_exact_score': 0.40028724799007476,\n",
       " 'f1_tokenwise_score': 0.397687590370067,\n",
       " 'f1_partial_score': 0.5159440267362017,\n",
       " 'f1_heuristic_score': 0.5273362748620107,\n",
       " 'f1_exact_brute_force_score': 0.40028724799007476,\n",
       " 'f1_article_tokenwise_score': 0.4981722776468529,\n",
       " 'f1_article_tokenwise_r_score': 0.5439488370573863,\n",
       " 'f1_positional_article_tokenwise_score': 0.7168747609545078}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_exact_score': 0.40028724799007476,\n",
       " 'f1_tokenwise_score': 0.397687590370067,\n",
       " 'f1_partial_score': 0.5159440267362017,\n",
       " 'f1_heuristic_score': 0.5273362748620107,\n",
       " 'f1_exact_brute_force_score': 0.40028724799007476,\n",
       " 'f1_article_tokenwise_score': 0.24908613882342645,\n",
       " 'f1_article_tokenwise_r_score': 0.5439488370573863,\n",
       " 'f1_positional_article_tokenwise_score': 0.7168747609545078}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_exact_score': 0.40028724799007476,\n",
       " 'f1_tokenwise_score': 0.397687590370067,\n",
       " 'f1_partial_score': 0.5159440267362017,\n",
       " 'f1_heuristic_score': 0.5273362748620107,\n",
       " 'f1_exact_brute_force_score': 0.40028724799007476,\n",
       " 'f1_article_tokenwise_score': 0.750468016406623,\n",
       " 'f1_article_tokenwise_r_score': 0.772280035777206,\n",
       " 'f1_positional_article_tokenwise_score': 0.7168747609545078}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'span' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e9490c99a5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'span' is not iterable"
     ]
    }
   ],
   "source": [
    "test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['D'][0].tag_ in test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['C'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RegulatoryInstr', 'RegulatoryInstr']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['C'][0].tokens[0].get_token_tags(annotators = 'annotators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_span.tag_ in cur_tok.get_token_spans(annotators = 'annotators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Tokens'][8].get_token_spans(annotators = 'annotators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Tokens'][8].get_token_spans(annotators = 'annotators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[span id:D118 annotator:D layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements, span id:D121 annotator:D layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:123 stop:131 text:hydrogen]\n",
      "\n",
      "[span id:C252 annotator:C layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements, span id:C255 annotator:C layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:172 stop:191 text:hydrogen components]\n",
      "\n",
      "\n",
      "[start:247 stop:259 text:requirements tag_count:1, start:123 stop:131 text:hydrogen tag_count:2]\n",
      "\n",
      "[start:247 stop:259 text:requirements tag_count:1, start:172 stop:180 text:hydrogen tag_count:2, start:181 stop:191 text:components tag_count:1]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "a =  [3,6]\n",
    "b = [3,6]\n",
    "\n",
    "a_l = []\n",
    "\n",
    "b_l = []\n",
    "\n",
    "\n",
    "for i in a:\n",
    "    a_l.append(test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['D'][i])\n",
    "    \n",
    "for i in b:\n",
    "    b_l.append(test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['C'][i])\n",
    "    \n",
    "a_l_t = list(chain.from_iterable([s.tokens for s in a_l]))\n",
    "b_l_t = list(chain.from_iterable([s.tokens for s in b_l]))\n",
    "\n",
    "print(a_l)\n",
    "print()\n",
    "print(b_l)\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(a_l_t)\n",
    "print()\n",
    "print(b_l_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[start:247 stop:259 text:requirements tag_count:1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "list((Counter(a_l_t) & Counter(b_l_t)).elements())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dict(Counter(a_l_t) & Counter(b_l_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[span id:D118 annotator:D layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements, span id:C252 annotator:C layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements, span id:CUR368 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in test:\n",
    "    print(t.spans)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'itertools.chain' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-935c373b1ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_l\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'itertools.chain' has no len()"
     ]
    }
   ],
   "source": [
    "len((Counter(a_l) & Counter(b_l)).elements())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-41ea4732c16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_l\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a_l' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tp = list((Counter(a_l) & Counter(b_l)).elements())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[start:93 stop:98 text:motor tag_count:1,\n",
       " start:99 stop:107 text:vehicles tag_count:1,\n",
       " start:93 stop:98 text:motor tag_count:1,\n",
       " start:99 stop:107 text:vehicles tag_count:1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Curation'][4], test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Curation'][4]]\n",
    "list(chain.from_iterable([l.tokens for l in test]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[span id:D115 annotator:D layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:55 stop:67 text:requirements,\n",
       " span id:C249 annotator:C layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:55 stop:67 text:requirements,\n",
       " span id:CUR365 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:55 stop:67 text:requirements]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Tokens'][8].spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7fe6c2464310>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_tokens = test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Tokens']\n",
    "sp = test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Curation'][1]\n",
    "\n",
    "map(lambda tok: tok.add_span(sp), span_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[start:0 stop:7 text:article tag_count:0,\n",
       " start:7 stop:8 text:  tag_count:0,\n",
       " start:8 stop:9 text:1 tag_count:0,\n",
       " start:11 stop:18 text:subject tag_count:0,\n",
       " start:19 stop:25 text:matter tag_count:0,\n",
       " start:27 stop:31 text:this tag_count:0,\n",
       " start:32 stop:42 text:regulation tag_count:0,\n",
       " start:43 stop:54 text:establishes tag_count:0,\n",
       " start:55 stop:67 text:requirements tag_count:0,\n",
       " start:68 stop:71 text:for tag_count:0,\n",
       " start:72 stop:75 text:the tag_count:0,\n",
       " start:76 stop:89 text:type-approval tag_count:0,\n",
       " start:90 stop:92 text:of tag_count:0,\n",
       " start:93 stop:98 text:motor tag_count:0,\n",
       " start:99 stop:107 text:vehicles tag_count:0,\n",
       " start:108 stop:112 text:with tag_count:0,\n",
       " start:113 stop:119 text:regard tag_count:0,\n",
       " start:120 stop:122 text:to tag_count:0,\n",
       " start:123 stop:131 text:hydrogen tag_count:0,\n",
       " start:132 stop:142 text:propulsion tag_count:0,\n",
       " start:143 stop:146 text:and tag_count:0,\n",
       " start:147 stop:150 text:for tag_count:0,\n",
       " start:151 stop:154 text:the tag_count:0,\n",
       " start:155 stop:168 text:type-approval tag_count:0,\n",
       " start:169 stop:171 text:of tag_count:0,\n",
       " start:172 stop:180 text:hydrogen tag_count:0,\n",
       " start:181 stop:191 text:components tag_count:0,\n",
       " start:192 stop:195 text:and tag_count:0,\n",
       " start:196 stop:204 text:hydrogen tag_count:0,\n",
       " start:205 stop:212 text:systems tag_count:0,\n",
       " start:212 stop:213 text:. tag_count:0,\n",
       " start:214 stop:218 text:this tag_count:0,\n",
       " start:219 stop:229 text:regulation tag_count:0,\n",
       " start:230 stop:234 text:also tag_count:0,\n",
       " start:235 stop:246 text:establishes tag_count:0,\n",
       " start:247 stop:259 text:requirements tag_count:0,\n",
       " start:260 stop:263 text:for tag_count:0,\n",
       " start:264 stop:267 text:the tag_count:0,\n",
       " start:268 stop:280 text:installation tag_count:0,\n",
       " start:281 stop:283 text:of tag_count:0,\n",
       " start:284 stop:288 text:such tag_count:0,\n",
       " start:289 stop:299 text:components tag_count:0,\n",
       " start:300 stop:303 text:and tag_count:0,\n",
       " start:304 stop:311 text:systems tag_count:0,\n",
       " start:311 stop:312 text:. tag_count:0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "span id:CUR366 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_tokens = test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Tokens']\n",
    "sp = test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Curation'][1]\n",
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[span id:CUR366 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_id = 'CUR1'\n",
    "[tok.add_span(sp) for tok in span_tokens ]\n",
    "span_tokens[0].spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[start:0 stop:7 text:article tag_count:1,\n",
       " start:7 stop:8 text:  tag_count:1,\n",
       " start:8 stop:9 text:1 tag_count:1,\n",
       " start:11 stop:18 text:subject tag_count:1,\n",
       " start:19 stop:25 text:matter tag_count:1,\n",
       " start:27 stop:31 text:this tag_count:1,\n",
       " start:32 stop:42 text:regulation tag_count:1,\n",
       " start:43 stop:54 text:establishes tag_count:1,\n",
       " start:55 stop:67 text:requirements tag_count:1,\n",
       " start:68 stop:71 text:for tag_count:1,\n",
       " start:72 stop:75 text:the tag_count:1,\n",
       " start:76 stop:89 text:type-approval tag_count:1,\n",
       " start:90 stop:92 text:of tag_count:1,\n",
       " start:93 stop:98 text:motor tag_count:1,\n",
       " start:99 stop:107 text:vehicles tag_count:1,\n",
       " start:108 stop:112 text:with tag_count:1,\n",
       " start:113 stop:119 text:regard tag_count:1,\n",
       " start:120 stop:122 text:to tag_count:1,\n",
       " start:123 stop:131 text:hydrogen tag_count:1,\n",
       " start:132 stop:142 text:propulsion tag_count:1,\n",
       " start:143 stop:146 text:and tag_count:1,\n",
       " start:147 stop:150 text:for tag_count:1,\n",
       " start:151 stop:154 text:the tag_count:1,\n",
       " start:155 stop:168 text:type-approval tag_count:1,\n",
       " start:169 stop:171 text:of tag_count:1,\n",
       " start:172 stop:180 text:hydrogen tag_count:1,\n",
       " start:181 stop:191 text:components tag_count:1,\n",
       " start:192 stop:195 text:and tag_count:1,\n",
       " start:196 stop:204 text:hydrogen tag_count:1,\n",
       " start:205 stop:212 text:systems tag_count:1,\n",
       " start:212 stop:213 text:. tag_count:1,\n",
       " start:214 stop:218 text:this tag_count:1,\n",
       " start:219 stop:229 text:regulation tag_count:1,\n",
       " start:230 stop:234 text:also tag_count:1,\n",
       " start:235 stop:246 text:establishes tag_count:1,\n",
       " start:247 stop:259 text:requirements tag_count:1,\n",
       " start:260 stop:263 text:for tag_count:1,\n",
       " start:264 stop:267 text:the tag_count:1,\n",
       " start:268 stop:280 text:installation tag_count:1,\n",
       " start:281 stop:283 text:of tag_count:1,\n",
       " start:284 stop:288 text:such tag_count:1,\n",
       " start:289 stop:299 text:components tag_count:1,\n",
       " start:300 stop:303 text:and tag_count:1,\n",
       " start:304 stop:311 text:systems tag_count:1,\n",
       " start:311 stop:312 text:. tag_count:1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[span id:CUR366 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[span_ for span_  in span_tokens[1].spans if 'Curation' in span_.annotator]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[span id:CUR366 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_tokens[0].get_token_spans(annotators = 'Curation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[span id:D115 annotator:Fabian layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:55 stop:67 text:requirements,\n",
       " span id:D116 annotator:Fabian layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval,\n",
       " span id:D117 annotator:Fabian layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:155 stop:168 text:type-approval,\n",
       " span id:D118 annotator:Fabian layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements,\n",
       " span id:D119 annotator:Fabian layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_Other start:93 stop:107 text:motor vehicles,\n",
       " span id:D120 annotator:Fabian layer:Technologyandapplicationspecificity type:ApplicationSpecificity tag:App_LowCarbon start:123 stop:142 text:hydrogen propulsion,\n",
       " span id:D121 annotator:Fabian layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:123 stop:131 text:hydrogen,\n",
       " span id:D122 annotator:Fabian layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:172 stop:191 text:hydrogen components,\n",
       " span id:D123 annotator:Fabian layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:172 stop:180 text:hydrogen,\n",
       " span id:D124 annotator:Fabian layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:196 stop:212 text:hydrogen systems,\n",
       " span id:D125 annotator:Fabian layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:196 stop:204 text:hydrogen]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Tokens'][8]test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Fabian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[span id:C249 annotator:Onerva layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:55 stop:67 text:requirements,\n",
       " span id:C250 annotator:Onerva layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval,\n",
       " span id:C251 annotator:Onerva layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:155 stop:168 text:type-approval,\n",
       " span id:C252 annotator:Onerva layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements,\n",
       " span id:C253 annotator:Onerva layer:Technologyandapplicationspecificity type:ApplicationSpecificity tag:App_Other start:93 stop:107 text:motor vehicles,\n",
       " span id:C254 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:123 stop:142 text:hydrogen propulsion,\n",
       " span id:C255 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:172 stop:191 text:hydrogen components,\n",
       " span id:C256 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:196 stop:212 text:hydrogen systems,\n",
       " span id:C257 annotator:Onerva layer:Technologyandapplicationspecificity type:ApplicationSpecificity tag:App_LowCarbon start:268 stop:280 text:installation,\n",
       " span id:C258 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:289 stop:299 text:components,\n",
       " span id:C259 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:304 stop:311 text:systems]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Onerva']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[span id:CUR365 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:55 stop:67 text:requirements,\n",
       " span id:CUR366 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval,\n",
       " span id:CUR367 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:155 stop:168 text:type-approval,\n",
       " span id:CUR368 annotator:Curation layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements,\n",
       " span id:CUR369 annotator:Curation layer:Technologyandapplicationspecificity type:ApplicationSpecificity tag:App_Other start:93 stop:107 text:motor vehicles,\n",
       " span id:CUR370 annotator:Curation layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:123 stop:142 text:hydrogen propulsion,\n",
       " span id:CUR371 annotator:Curation layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:123 stop:131 text:hydrogen,\n",
       " span id:CUR372 annotator:Curation layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:172 stop:191 text:hydrogen components,\n",
       " span id:CUR373 annotator:Curation layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:172 stop:180 text:hydrogen,\n",
       " span id:CUR374 annotator:Curation layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:196 stop:212 text:hydrogen systems,\n",
       " span id:CUR375 annotator:Curation layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:196 stop:204 text:hydrogen,\n",
       " span id:CUR376 annotator:Curation layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:289 stop:299 text:components,\n",
       " span id:CUR377 annotator:Curation layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:304 stop:311 text:systems]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01']['Curation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1_exact_score        0.545455\n",
       "f1_tokenwise_score    0.428571\n",
       "f1_partial_score      0.545455\n",
       "f1_heuristic_score    0.545455\n",
       "Name: EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.df.loc['EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01'][13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_exact_score': 0.40028724799007476,\n",
       " 'f1_tokenwise_score': 0.397687590370067,\n",
       " 'f1_partial_score': 0.5159440267362017,\n",
       " 'f1_heuristic_score': 0.5273362748620107,\n",
       " 'f1_exact_brute_force_score': 0.40028724799007476,\n",
       " 'f1_article_tokenwise_score': 0.0,\n",
       " 'f1_article_tokenwise_r_score': 0.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span id:D115 annotator:Fabian layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:55 stop:67 text:requirements\n",
      "span id:C249 annotator:Onerva layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:55 stop:67 text:requirements\n",
      "\n",
      "span id:D116 annotator:Fabian layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval\n",
      "span id:C250 annotator:Onerva layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:76 stop:89 text:type-approval\n",
      "\n",
      "span id:D119 annotator:Fabian layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_Other start:93 stop:107 text:motor vehicles\n",
      "span id:C253 annotator:Onerva layer:Technologyandapplicationspecificity type:ApplicationSpecificity tag:App_Other start:93 stop:107 text:motor vehicles\n",
      "\n",
      "span id:D121 annotator:Fabian layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:123 stop:131 text:hydrogen\n",
      "span id:None annotator:Onerva layer:None type:None tag:None start:None stop:None text:None\n",
      "\n",
      "span id:D120 annotator:Fabian layer:Technologyandapplicationspecificity type:ApplicationSpecificity tag:App_LowCarbon start:123 stop:142 text:hydrogen propulsion\n",
      "span id:C254 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:123 stop:142 text:hydrogen propulsion\n",
      "\n",
      "span id:D117 annotator:Fabian layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:155 stop:168 text:type-approval\n",
      "span id:C251 annotator:Onerva layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:155 stop:168 text:type-approval\n",
      "\n",
      "span id:D123 annotator:Fabian layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:172 stop:180 text:hydrogen\n",
      "span id:None annotator:Onerva layer:None type:None tag:None start:None stop:None text:None\n",
      "\n",
      "span id:D122 annotator:Fabian layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:172 stop:191 text:hydrogen components\n",
      "span id:C255 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:172 stop:191 text:hydrogen components\n",
      "\n",
      "span id:D125 annotator:Fabian layer:Technologyandapplicationspecificity type:EnergySpecificity tag:Energy_LowCarbon start:196 stop:204 text:hydrogen\n",
      "span id:None annotator:Onerva layer:None type:None tag:None start:None stop:None text:None\n",
      "\n",
      "span id:D124 annotator:Fabian layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:196 stop:212 text:hydrogen systems\n",
      "span id:C256 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:196 stop:212 text:hydrogen systems\n",
      "\n",
      "span id:D118 annotator:Fabian layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements\n",
      "span id:C252 annotator:Onerva layer:Instrumenttypes type:InstrumentType tag:RegulatoryInstr start:247 stop:259 text:requirements\n",
      "\n",
      "span id:None annotator:Fabian layer:None type:None tag:None start:None stop:None text:None\n",
      "span id:C257 annotator:Onerva layer:Technologyandapplicationspecificity type:ApplicationSpecificity tag:App_LowCarbon start:268 stop:280 text:installation\n",
      "\n",
      "span id:None annotator:Fabian layer:None type:None tag:None start:None stop:None text:None\n",
      "span id:C258 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:289 stop:299 text:components\n",
      "\n",
      "span id:None annotator:Fabian layer:None type:None tag:None start:None stop:None text:None\n",
      "span id:C259 annotator:Onerva layer:Technologyandapplicationspecificity type:TechnologySpecificity tag:Tech_LowCarbon start:304 stop:311 text:systems\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.d03_inter_annotator_agreement.span_matching import create_tuples_pygamma\n",
    "\n",
    "rep = repository.from_repository_name('EU_32009R0079_Title_0_Chapter_0_Section_0_Article_01')\n",
    "spanlist = test_evaluator.get_span_list(conditional_rep = rep, annotators = 'annotators')\n",
    "for tu in create_tuples_pygamma(spanlist):\n",
    "    for i in tu:\n",
    "        print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " collections.Counter(self.tokens) & collections.Counter(other.tokens)\n",
    "            num_same = sum(common.values())\n",
    "\n",
    "            if num_same == 0:\n",
    "                return 0\n",
    "\n",
    "            precision = 1.0 * num_same / len(self.tokens)\n",
    "            recall = 1.0 * num_same / len(other.tokens)\n",
    "            f1 = (2*precision * recall) / (precision + recall)\n",
    "            return f1\n",
    "        else:\n",
    "            raise ValueError('None of the mentioned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_list = create_tuples_pygamma(spanlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_tokenwise(create_tuples_pygamma(spanlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([n_tuple[0] for n_tuple in tu_list if n_tuple[0].tag_ != None ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in test_evaluator.df.iterrows():\n",
    "    print(row.name)\n",
    "    print(len(row['Curation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all the non curated articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.df_non_curated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chech if there are finished articles where there are less than 2 finsihed annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.df[test_evaluator.df.apply(lambda x: len(x['Finished_Annotators'])  < 2,axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test small helpfer functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row to spanlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_span_list(test_evaluator.df.loc['EU_32009R0397_Title_0_Chapter_0_Section_0_Article_02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = repository.from_repository_name('EU_32009R0397_Title_0_Chapter_0_Section_0_Article_02')\n",
    "test_evaluator.get_span_list(repo, columns = 'annotators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _get_score_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TO DO: TEST SCORE OF A FEW KNOWN SCORES **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe nan scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_evaluator.df[test_evaluator.df['f1_heuristic_score'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = repository.from_repository_name('EU_32009R0079_Title_0_Chapter_0_Section_0_Article_14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglist = test_evaluator.get_span_list(repo, columns = 'annotators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.get_score_spanlist(taglist, scoring_metric = 'f1_heuristic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test weighting of total score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'f1_exact'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nlp_spark/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'f1_exact'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-52874a992a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscoring_metric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscoring_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscoring_metric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nlp_spark/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_spark/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'f1_exact'"
     ]
    }
   ],
   "source": [
    "for scoring_metric in scoring_metrics:\n",
    "    print(scoring_metric, test_evaluator.df[scoring_metric].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually weight by tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scoring_metric in scoring_metrics:\n",
    "    print(scoring_metric, (test_evaluator.df.Tokens.map(len) * test_evaluator.df[scoring_metric + '_score']).sum()/test_evaluator.df.Tokens.map(len).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually weight by spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.d03_inter_annotator_agreement.inter_annotator_agremment import _get_span_count_in_row\n",
    "for scoring_metric in scoring_metrics:\n",
    "\n",
    "    test_evaluator.df['Span_len'] = test_evaluator.df.apply(lambda row: _get_span_count_in_row(row, cols = row['Finished_Annotators']), axis = 1)\n",
    "    print((test_evaluator.df['Span_len'] * test_evaluator.df[scoring_metric + '_score']).sum()/test_evaluator.df['Span_len'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.get_total_score_df(weight_by = 'Spans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test individual score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if individual score averaged yields total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sco in scoring_metrics:\n",
    "    tokens = 0\n",
    "    res = 0\n",
    "\n",
    "    sco = sco + '_score'\n",
    "    for ann in test_evaluator.annotators:\n",
    "        tok = len(list(chain.from_iterable(test_evaluator.df[test_evaluator.df.apply(lambda x: ann in x['Finished_Annotators'],axis=1)]['Tokens'])))\n",
    "\n",
    "        res += test_evaluator.get_total_score_df(annotator = ann)[sco] * tok\n",
    "        tokens += tok\n",
    "    print(sco, res / tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing get_score_spanlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dir = repository.from_repository_name('EU_32018R1999_Title_0_Chapter_7_Section_3_Article_43')\n",
    "span_list = test_evaluator.get_span_list(test_dir, ['Alisha', 'Fride'])\n",
    "len(span_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.get_score_spanlist(span_list, 'f1_exact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = repository.from_repository_name('EU_32018R1999_Title_0_Chapter_7_Section_3_Article_56')\n",
    "span_list1 = test_evaluator.get_span_list(test_dir, ['Alisha', 'Fride'])\n",
    "len(span_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.get_score_spanlist(span_list1, 'f1_exact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_list2 = span_list + span_list1\n",
    "test_evaluator.get_score_spanlist(span_list2, 'f1_exact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_evaluator.get_score_spanlist(span_list, 'f1_exact') * len(span_list) + test_evaluator.get_score_spanlist(span_list1, 'f1_exact') * len(span_list1)) / (len(span_list) + len(span_list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if this is also true for a long spanlist of multiple repositorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#itnitialize test evaluator again, this time with debug \n",
    "test_evaluator_debug = Inter_Annotator_Agreement(stat_df, DEBUG = True)\n",
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial', 'f1_heuristic']\n",
    "\n",
    "test_evaluator_debug.append_total_score_per_article(scoring_metrics)\n",
    "test_evaluator_debug.append_score_to_curation(scoring_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator_debug.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = repository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all tags of fabian and curation\n",
    "span_list_Fabian = test_evaluator_debug.get_span_list(test_dir, ['Curation', 'Fabian'])\n",
    "set([spn.rep for spn in span_list_Fabian if spn.annotator == 'Fabian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all tags of Fride and curation\n",
    "span_list_fride = test_evaluator_debug.get_span_list(test_dir, ['Curation', 'Fride'])\n",
    "set([spn.rep for spn in span_list_fride if spn.annotator == 'Fride'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigh_by = 'Spans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter annotator agreement to curation\n",
    "\n",
    "test_evaluator_debug.get_to_curation_score(weight_by = weigh_by)['Fabian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sco in scoring_metrics:\n",
    "    print(sco, test_evaluator_debug.get_score_spanlist(test_dir, columns = ['Curation', 'Fabian'], scoring_metric = sco, weight_by = weigh_by))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator_debug.get_to_curation_score(weight_by = weigh_by)['Fride']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sco in scoring_metrics:\n",
    "    print(sco, test_evaluator_debug.get_score_spanlist(test_dir, columns = ['Curation', 'Fride'], scoring_metric = sco, weight_by = weigh_by))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually weight Fride by Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be the same as above\n",
    "Fride_df = test_evaluator_debug.df[test_evaluator_debug.df.apply(lambda x : len(x['Fride']) >=2 and len(x['Curation']) >=2,axis=1)]\n",
    "scoring_metric = 'f1_partial'\n",
    "score = 0\n",
    "total_count = 0\n",
    "\n",
    "for i, row in Fride_df.iterrows():\n",
    "    span_list = list(chain.from_iterable(row[['Fride', 'Curation']]))\n",
    "    score_article = _get_score_article(span_list, scoring_metric, finished_annotators = ['Fride', 'Curation'])\n",
    "    score += score_article * len(span_list)\n",
    "    total_count += len(span_list)\n",
    "print(score/total_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually weight Fabian by Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be the same as above\n",
    "Fride_df = test_evaluator_debug.df[test_evaluator_debug.df.apply(lambda x : len(x['Fabian']) >=2 and len(x['Curation']) >=2,axis=1)]\n",
    "scoring_metric = 'f1_partial'\n",
    "score = 0\n",
    "total_count = 0\n",
    "\n",
    "for i, row in Fride_df.iterrows():\n",
    "    span_list = list(chain.from_iterable(row[['Fabian', 'Curation']]))\n",
    "    score_article = _get_score_article(span_list, scoring_metric, finished_annotators = ['Fabian', 'Curation'])\n",
    "    score += score_article * len(span_list)\n",
    "    total_count += len(span_list)\n",
    "print(score/total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test get_score_anotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator_debug.get_total_score_df(annotator = 'Fride', weight_by = 'Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotator_df = test_evaluator_debug.df[test_evaluator_debug.df.apply(lambda x: 'Fride' in x['Finished_Annotators'],axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1 partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be the same as this one\n",
    "sum = 0\n",
    "for i, row in annotator_df.iterrows():\n",
    "    sum += len(row['Tokens']) * row['f1_partial_score']\n",
    "sum/len(list(chain.from_iterable(annotator_df['Tokens'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1 tokenwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i, row in annotator_df.iterrows():\n",
    "    sum += len(row['Tokens']) * row['f1_tokenwise_score']\n",
    "sum/len(list(chain.from_iterable(annotator_df['Tokens'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_evaluator_debug.df## Test heuristic_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator_debug.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3\n",
    "test_dir = test_evaluator_debug.df['Curation'][index][1].rep\n",
    "span_list = test_evaluator_debug.get_span_list(test_dir, columns = 'annotators')\n",
    "from src.d03_inter_annotator_agreement.scoring_functions import f1_heuristic\n",
    "test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[spn for spn in span_list if spn.annotator == 'Alisha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[spn for spn in span_list if spn.annotator == 'Fride']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_heuristic(span_list,( 'Alisha', 'Fride'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating by hand:\n",
    "98/133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pygamma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom scoring matrix\n",
    "category_list, cat_dissimilarity_matrix = create_scoring_matrix(os.path.join(ROOT_DIR,'src/experiment_utils/tag_set.json'),  soft_tagset_dissimilarity = True)\n",
    "cat_dissimilarity_matrix_test = np.ones((len(category_list), len(category_list)))\n",
    "np.fill_diagonal(cat_dissimilarity_matrix_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare influence of soft dissimilarity\n",
    "#without matrix\n",
    "test_evaluator = Inter_Annotator_Agreement(stat_df)\n",
    "test_evaluator.keep_only_finished_articles()\n",
    "scoring_metrics = ['pygamma']\n",
    "test_evaluator.append_total_score_per_article(scoring_metrics)\n",
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with matrix\n",
    "test_evaluator = Inter_Annotator_Agreement(stat_df)\n",
    "test_evaluator.keep_only_finished_articles()\n",
    "scoring_metrics = ['pygamma']\n",
    "test_evaluator.append_total_score_per_article(scoring_metrics, category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix)\n",
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with test matrix\n",
    "test_evaluator = Inter_Annotator_Agreement(stat_df)\n",
    "test_evaluator.keep_only_finished_articles()\n",
    "scoring_metrics = ['pygamma']\n",
    "test_evaluator.append_total_score_per_article(scoring_metrics, category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix_test)\n",
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with test matrix\n",
    "test_evaluator = Inter_Annotator_Agreement(stat_df, front_and_whereas = False, DEBUG = False)\n",
    "test_evaluator.keep_only_finished_articles()\n",
    "scoring_metrics = ['pygamma']\n",
    "test_evaluator.append_total_score_per_article_parallel(scoring_metrics, category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix_test)\n",
    "test_evaluator.get_total_score_df(weight_by = 'no_weighting')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare influence of soft dissimilarity\n",
    "#without matrix\n",
    "test_evaluator = Inter_Annotator_Agreement(stat_df)\n",
    "test_evaluator.keep_only_finished_articles()\n",
    "scoring_metrics = ['pygamma']\n",
    "test_evaluator.append_score_to_curation(scoring_metrics)\n",
    "test_evaluator.get_to_curation_score(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare influence of soft dissimilarity\n",
    "#without matrix\n",
    "test_evaluator = Inter_Annotator_Agreement(stat_df)\n",
    "test_evaluator.keep_only_finished_articles()\n",
    "scoring_metrics = ['pygamma']\n",
    "test_evaluator.append_score_to_curation(scoring_metrics, category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix)\n",
    "test_evaluator.get_to_curation_score(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare influence of soft dissimilarity\n",
    "#without matrix\n",
    "test_evaluator = Inter_Annotator_Agreement(stat_df)\n",
    "test_evaluator.keep_only_finished_articles()\n",
    "scoring_metrics = ['pygamma']\n",
    "test_evaluator.append_score_to_curation(scoring_metrics, category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix_test)\n",
    "test_evaluator.get_to_curation_score(weight_by = 'no_weighting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial', 'f1_heuristic']\n",
    "scoring_metrics.sort()\n",
    "test_evaluator.append_total_score_per_article(scoring_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list, cat_dissimilarity_matrix = create_scoring_matrix(os.path.join(ROOT_DIR,'src/experiment_utils/tag_set.json'),  soft_tagset_dissimilarity = True, soft_layer_dissimilarity = False)\n",
    "test_evaluator.append_total_score_per_article(scoring_metrics = ['pygamma'], category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial', 'f1_heuristic']\n",
    "category_list, cat_dissimilarity_matrix = create_scoring_matrix(os.path.join(ROOT_DIR,'src/experiment_utils/tag_set.json'),  soft_tagset_dissimilarity = True, soft_layer_dissimilarity = False)\n",
    "test_evaluator.append_score_to_curation(scoring_metrics = scoring_metrics, category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix)\n",
    "\n",
    "#test_evaluator.append_score_to_curation(scoring_metrics = scoring_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test difference in pygamma matching algo for f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial']\n",
    "category_list, cat_dissimilarity_matrix = create_scoring_matrix(os.path.join(ROOT_DIR,'src/experiment_utils/tag_set.json'),  soft_tagset_dissimilarity = True, soft_layer_dissimilarity = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first add no dissimilary matrix for the pygamma matching algo\n",
    "\n",
    "test_evaluator_no_matrix = Inter_Annotator_Agreement(stat_df, front_and_whereas = False, DEBUG = DEBUG)\n",
    "test_evaluator_no_matrix.keep_only_finished_articles()\n",
    "\n",
    "test_evaluator_no_matrix.append_total_score_per_article(scoring_metrics)\n",
    "test_evaluator_no_matrix.append_score_to_curation(scoring_metrics)\n",
    "\n",
    "print('Total score:')\n",
    "print(test_evaluator_no_matrix.get_total_score_df(weight_by = 'no_weighting'))\n",
    "print('Curation score:')      \n",
    "print(test_evaluator_no_matrix.get_to_curation_score(weight_by = 'no_weighting'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add the categories for the pygamma matching algo\n",
    "test_evaluator_scoring_matrix = Inter_Annotator_Agreement(stat_df, front_and_whereas = False, DEBUG = DEBUG)\n",
    "test_evaluator_scoring_matrix.keep_only_finished_articles()\n",
    "\n",
    "test_evaluator_scoring_matrix.append_total_score_per_article(scoring_metrics, category_list_matching = category_list, cat_dissimilarity_matrix_matching = cat_dissimilarity_matrix)\n",
    "test_evaluator_scoring_matrix.append_score_to_curation(scoring_metrics, category_list_matching = category_list, cat_dissimilarity_matrix_matching = cat_dissimilarity_matrix)\n",
    "\n",
    "print('Total score:')\n",
    "print(test_evaluator_scoring_matrix.get_total_score_df(weight_by = 'no_weighting'))\n",
    "print('Curation score:')      \n",
    "print(test_evaluator_scoring_matrix.get_to_curation_score(weight_by = 'no_weighting'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The scores are very simmilar, feeding no dissimilarity matrix yields a higher score!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if get score spanlist yields the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_by = 'no_weighting'\n",
    "scoring_metric = 'f1_exact'\n",
    "roud_to = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(scoring_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_number = 0\n",
    "for i, col in test_evaluator.df.iterrows():\n",
    "    if (row_number % 20 == 0):\n",
    "        print(row_number)\n",
    "    \n",
    "    name= col.name\n",
    "    test_rep = repository.from_repository_name(name)\n",
    "    finished_annotators = test_evaluator.df.loc[name]['Finished_Annotators']\n",
    "\n",
    "    test_span_list_O = test_evaluator.get_span_list(test_rep, columns = ['Curation', finished_annotators[0]])\n",
    "    test_span_list_1 = test_evaluator.get_span_list(test_rep, columns = ['Curation', finished_annotators[1]])\n",
    "    \n",
    "    if len(list(set([spn.annotator for spn in test_span_list_O]))) < 2 or len(list(set([spn.annotator for spn in test_span_list_1]))) < 2:\n",
    "        continue\n",
    "\n",
    "    score_df_0 = test_evaluator.df.loc[name][(finished_annotators[0] + '_to_curation')]\n",
    "    score_df_1 = test_evaluator.df.loc[name][(finished_annotators[1] + '_to_curation')]\n",
    "\n",
    "    score_array_0 = np.zeros(len(scoring_metrics))\n",
    "    score_array_1 = np.zeros(len(scoring_metrics))\n",
    "\n",
    "        \n",
    "    for i, sco in enumerate(sorted(scoring_metrics)):\n",
    "\n",
    "        score_array_0[i] = test_evaluator.get_score_spanlist(test_span_list_O, weight_by = weight_by, scoring_metric = sco)\n",
    "        score_array_1[i] = test_evaluator.get_score_spanlist(test_span_list_1, weight_by = weight_by, scoring_metric = sco)\n",
    "        \n",
    "    assert (np.round(score_df_0,roud_to) == np.round(score_array_0,roud_to)).all(), f\" score of {col} of annotator {finished_annotators[0]}: df score {np.round(score_df_0,roud_to)} spanlist score {np.round(score_array_0,roud_to)}\"\n",
    "    assert (np.round(score_df_1,roud_to) == np.round(score_array_1,roud_to)).all(), f\" score of {col} of annotator {finished_annotators[1]}: df score {np.round(score_df_1,roud_to)} spanlist score {np.round(score_array_1,roud_to)}\"\n",
    "    row_number +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'EU_32006L0066_Title_0_Chapter_0_Section_0_Article_11'\n",
    "test_rep = repository.from_repository_name(name)\n",
    "finished_annotators = test_evaluator.df.loc[name]['Finished_Annotators']\n",
    "\n",
    "test_span_list_O = test_evaluator.get_span_list(test_rep, columns = ['Curation', finished_annotators[0]])\n",
    "test_span_list_1 = test_evaluator.get_span_list(test_rep, columns = ['Curation', finished_annotators[1]])\n",
    "\n",
    "score_df_0 = test_evaluator.df.loc[name][(finished_annotators[0] + '_to_curation')]\n",
    "score_df_1 = test_evaluator.df.loc[name][(finished_annotators[1] + '_to_curation')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(scoring_metrics))\n",
    "finished_annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sco in sorted(scoring_metrics):\n",
    "    print(test_evaluator.get_score_spanlist(test_span_list_1, weight_by = 'no_weighting', scoring_metric = sco))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sco in sorted(scoring_metrics):\n",
    "    print(test_evaluator._get_score_(test_span_list_1, weight_by = 'no_weighting', scoring_metric = sco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_score_article(test_span_list_1, 'f1_partial', ['Curation', finished_annotators[1]], category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_curation_annotator_score(test_evaluator.df.loc[name], sorted(scoring_metrics), sorted(scoring_metrics), finished_annotators[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator_11 = Inter_Annotator_Agreement(stat_df.loc[[name]], front_and_whereas = False)\n",
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial', 'f1_heuristic']\n",
    "category_list, cat_dissimilarity_matrix = create_scoring_matrix(os.path.join(ROOT_DIR,'src/experiment_utils/tag_set.json'),  soft_tagset_dissimilarity = True, soft_layer_dissimilarity = False)\n",
    "test_evaluator_11.append_score_to_curation(scoring_metrics = scoring_metrics, category_list = category_list, cat_dissimilarity_matrix = cat_dissimilarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator_11.df.loc[name]['Onerva_to_curation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if adding multiple scores yields the same as adding only a single score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact', 'f1_tokenwise','f1_partial', 'f1_heuristic']\n",
    "\n",
    "weighting_methods = ['no_weighting', 'Spans', 'Tokens']\n",
    "\n",
    "annotators = ['Fabian', 'Onerva', 'Fride', 'Alisha']\n",
    "tot_results = test_evaluator.append_score_to_curation(scoring_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for wei_meth in weighting_methods:\n",
    "    print(f\"  \\n \\n elaborating {wei_meth} ... \\n \\n\")\n",
    "    tot_results = test_evaluator.get_to_curation_score(weight_by = wei_meth)\n",
    "    \n",
    "    for sco_meth in scoring_metrics:\n",
    "        print(f\" elaborating {sco_meth} ...\")\n",
    "        test_evaluator_ind = Inter_Annotator_Agreement(stat_df, DEBUG = DEBUG)\n",
    "        test_evaluator_ind.keep_only_finished_articles()\n",
    "        sco_meths = [sco_meth]\n",
    "        test_evaluator_ind.append_score_to_curation(sco_meths)\n",
    "        ind_results = test_evaluator_ind.get_to_curation_score(weight_by = wei_meth)\n",
    "        \n",
    "        for ann in annotators:\n",
    "            assert np.round(tot_results[ann][sco_meth],3) == np.round(ind_results[ann][sco_meth],3), f\"tot results of annotator {ann} was {tot_results[ann][sco_meth]} and ind result was {ind_results[ann][sco_meth]}\"\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if this also works when adding the scores separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact', 'f1_tokenwise']\n",
    "\n",
    "weighting_methods = ['no_weighting', 'Spans', 'Tokens']\n",
    "\n",
    "annotators = ['Fabian', 'Onerva', 'Fride', 'Alisha']\n",
    "tot_results = test_evaluator.append_score_to_curation(scoring_metrics)\n",
    "scoring_metrics = ['f1_partial', 'f1_heuristic']\n",
    "tot_results = test_evaluator.append_score_to_curation(scoring_metrics)\n",
    "\n",
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial', 'f1_heuristic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wei_meth in weighting_methods:\n",
    "    print(f\"  \\n \\n elaborating {wei_meth} ... \\n \\n\")\n",
    "    tot_results = test_evaluator.get_to_curation_score(weight_by = wei_meth)\n",
    "    \n",
    "    for sco_meth in scoring_metrics:\n",
    "        print(f\" elaborating {sco_meth} ...\")\n",
    "        test_evaluator_ind = Inter_Annotator_Agreement(stat_df, DEBUG = DEBUG)\n",
    "        test_evaluator_ind.keep_only_finished_articles()\n",
    "        sco_meths = [sco_meth]\n",
    "        test_evaluator_ind.append_score_to_curation(sco_meths)\n",
    "        ind_results = test_evaluator_ind.get_to_curation_score(weight_by = wei_meth)\n",
    "        \n",
    "        for ann in annotators:\n",
    "            assert np.round(tot_results[ann][sco_meth],3) == np.round(ind_results[ann][sco_meth],3), f\"tot results of annotator {ann} was {tot_results[ann][sco_meth]} and ind result was {ind_results[ann][sco_meth]}\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tot_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial', 'f1_heuristic']\n",
    "for sco_meth in scoring_metrics:\n",
    "    print(f\" elaborating {sco_meth} ...\")\n",
    "    test_evaluator_ind = Inter_Annotator_Agreement(stat_df, DEBUG = DEBUG)\n",
    "    test_evaluator_ind.keep_only_finished_articles()\n",
    "    scoring_metrics = [sco_meth]\n",
    "    test_evaluator_ind.append_score_to_curation(scoring_metrics)\n",
    "    ind_results = test_evaluator_ind.get_to_curation_score(weight_by = 'no_weighting')\n",
    "    print(ind_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= [False, False, False]\n",
    "max(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parallel implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial', 'f1_heuristic', 'pygamma']\n",
    "test_evaluator = Inter_Annotator_Agreement(stat_df, front_and_whereas = False)\n",
    "test_evaluator.append_total_score_per_article(scoring_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = ['f1_exact', 'f1_tokenwise', 'f1_partial', 'f1_heuristic', 'pygamma']\n",
    "test_evaluator_parallel = Inter_Annotator_Agreement(stat_df, front_and_whereas = False)\n",
    "test_evaluator_parallel.append_total_score_per_article(scoring_metrics, parallel = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator.get_total_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n[GCC 9.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1357e5d6940800ae93a3fd23162ea93a77a6c89f02c68504dcfa719d1ffeee2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
