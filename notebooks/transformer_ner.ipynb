{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75223b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# New notebook file for transformer-based NER\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Add project root to path (same as your current notebook)\n",
    "sys.path.insert(0, '..')\n",
    "from src.experiment_utils.helper_classes import span, repository\n",
    "from src.d02_corpus_statistics.corpus import Corpus\n",
    "from definitions import ROOT_DIR\n",
    "\n",
    "# Transformer-specific imports\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification, \n",
    "    AutoTokenizer,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f53dd",
   "metadata": {},
   "source": [
    "2.1 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0af4a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataframe (same as your current notebook)\n",
    "dataframe_dir = os.path.join(ROOT_DIR, 'data/preprocessed_dataframe.pkl')\n",
    "stat_df = pd.read_pickle(dataframe_dir)\n",
    "\n",
    "# Create corpus object\n",
    "corpus = Corpus(stat_df)\n",
    "\n",
    "# Create a repository to match all documents\n",
    "all_docs = repository()\n",
    "\n",
    "# Extract Actor spans\n",
    "actor_spans = corpus.get_span_list(\n",
    "    conditional_rep=all_docs, \n",
    "    annotators='Curation', \n",
    "    item='feature', \n",
    "    value='Actor'\n",
    ")\n",
    "\n",
    "# Define the tags we're focusing on (same as your current notebook)\n",
    "sufficient_tags = [\n",
    "    'Addressee_default',\n",
    "    'Addressee_sector',\n",
    "    'Authority_default',\n",
    "    'Authority_monitoring',\n",
    "    'Addressee_monitored',\n",
    "    'Authority_legislative',\n",
    "    'Addressee_resource'\n",
    "]\n",
    "\n",
    "# Extract spans for each tag type (similar to your current approach)\n",
    "all_spans = []\n",
    "\n",
    "for tag in sufficient_tags:\n",
    "    # Get spans for this specific tag\n",
    "    tag_spans = corpus.get_span_list(\n",
    "        conditional_rep=all_docs,\n",
    "        annotators='Curation',\n",
    "        item='tag',\n",
    "        value=tag\n",
    "    )\n",
    "    \n",
    "    # Extract relevant information and add context\n",
    "    for span in tag_spans:\n",
    "        # Get the document text\n",
    "        doc_text = corpus.df.loc[span.rep.index_name, 'Text']\n",
    "        \n",
    "        # Calculate context window boundaries - use larger context for transformers\n",
    "        start_ctx = max(0, span.start - 100)\n",
    "        end_ctx = min(len(doc_text), span.stop + 100)\n",
    "        \n",
    "        # Extract context\n",
    "        context = doc_text[start_ctx:end_ctx]\n",
    "        \n",
    "        all_spans.append({\n",
    "            'text': span.text,\n",
    "            'tag': span.tag,\n",
    "            'document': span.rep.index_name,\n",
    "            'context': context,\n",
    "            'start': span.start - start_ctx,  # Adjust start position for the context\n",
    "            'stop': span.stop - start_ctx     # Adjust stop position for the context\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "spans_df = pd.DataFrame(all_spans)\n",
    "\n",
    "# Add high-level category (similar to your approach)\n",
    "spans_df['high_level_category'] = spans_df['tag'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfac4b2",
   "metadata": {},
   "source": [
    "2.2 Preparing Data for Transformer-based NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292bd0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4673 samples\n",
      "Test set: 1169 samples\n",
      "Created 4673 training examples\n",
      "Created 1169 test examples\n",
      "\n",
      "Sample example:\n",
      "Text: 'rated undertaking;\n",
      "(b)\n",
      "to monitor communications...'\n",
      "Entities:\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets with stratification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    spans_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=spans_df['tag']\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "# Define your NER label set\n",
    "label_list = [\"O\", \"B-AUTHORITY\", \"I-AUTHORITY\", \"B-ADDRESSEE\", \"I-ADDRESSEE\", \"B-SECTOR\", \"I-SECTOR\"]\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "\n",
    "def convert_to_transformer_ner_format(df):\n",
    "    \"\"\"Convert dataframe to the format expected by HuggingFace transformers for NER\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        context = row['context']\n",
    "        span_text = row['text']\n",
    "        start_pos = row['start']\n",
    "        end_pos = row['stop']\n",
    "        \n",
    "        # Determine entity label\n",
    "        if row['high_level_category'] == 'Authority':\n",
    "            entity_type = \"AUTHORITY\"\n",
    "        elif row['tag'] == 'Addressee_sector':\n",
    "            entity_type = \"SECTOR\"\n",
    "        else:\n",
    "            entity_type = \"ADDRESSEE\"\n",
    "            \n",
    "        # Create IOB tags for each token\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        \n",
    "        # Tokenize the context\n",
    "        for i, char in enumerate(context):\n",
    "            # Very simple character-level tokenization for demonstration\n",
    "            # In practice, you'd use the model's tokenizer\n",
    "            tokens.append(char)\n",
    "            \n",
    "            # Assign NER tags\n",
    "            if i == start_pos:\n",
    "                ner_tags.append(f\"B-{entity_type}\")\n",
    "            elif start_pos < i < end_pos:\n",
    "                ner_tags.append(f\"I-{entity_type}\")\n",
    "            else:\n",
    "                ner_tags.append(\"O\")\n",
    "        \n",
    "        examples.append({\n",
    "            \"tokens\": tokens, \n",
    "            \"ner_tags\": ner_tags,\n",
    "            \"text\": context\n",
    "        })\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Convert data for transformers\n",
    "train_examples = convert_to_transformer_ner_format(train_df)\n",
    "test_examples = convert_to_transformer_ner_format(test_df)\n",
    "\n",
    "print(f\"Created {len(train_examples)} training examples\")\n",
    "print(f\"Created {len(test_examples)} test examples\")\n",
    "\n",
    "# Display a sample example\n",
    "sample = train_examples[0]\n",
    "print(\"\\nSample example:\")\n",
    "entity_spans = []\n",
    "current_entity = None\n",
    "for i, (token, tag) in enumerate(zip(sample[\"tokens\"][:50], sample[\"ner_tags\"][:50])):\n",
    "    if tag.startswith(\"B-\"):\n",
    "        if current_entity:\n",
    "            entity_spans.append(current_entity)\n",
    "        current_entity = {\"start\": i, \"text\": token, \"tag\": tag[2:]}\n",
    "    elif tag.startswith(\"I-\") and current_entity:\n",
    "        current_entity[\"text\"] += token\n",
    "    elif current_entity:\n",
    "        entity_spans.append(current_entity)\n",
    "        current_entity = None\n",
    "\n",
    "print(f\"Text: '{''.join(sample['tokens'][:50])}...'\")\n",
    "print(\"Entities:\")\n",
    "for entity in entity_spans:\n",
    "    print(f\"  - '{entity['text']}' ({entity['tag']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa38532",
   "metadata": {},
   "source": [
    "2.3 Tokenizer and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4e3aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4673/4673 [00:01<00:00, 2503.54 examples/s]\n",
      "Map: 100%|██████████| 1169/1169 [00:00<00:00, 2537.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Use a pre-trained transformer model\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenize examples and align labels with wordpiece tokens\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # For the first token of a word, we keep the label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            # For other tokens of the same word, add the same label\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Convert lists to Dataset objects\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "test_dataset = Dataset.from_list(test_examples)\n",
    "\n",
    "# Apply tokenization and label alignment\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3917213",
   "metadata": {},
   "source": [
    "2.4 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69fb2d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m     model_checkpoint,\n\u001b[1;32m      4\u001b[0m     id2label\u001b[38;5;241m=\u001b[39mid2label,\n\u001b[1;32m      5\u001b[0m     label2id\u001b[38;5;241m=\u001b[39mlabel2id,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./transformer_ner_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize Trainer\u001b[39;00m\n\u001b[1;32m     22\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForTokenClassification(tokenizer)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# Fine-tune a pre-trained transformer model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./transformer_ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the transformer model...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./transformer_ner_model/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c66e5f",
   "metadata": {},
   "source": [
    "2.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model predictions to NER tags\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    \n",
    "    true_labels = [[] for _ in range(batch_size)]\n",
    "    pred_labels = [[] for _ in range(batch_size)]\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                true_labels[batch_idx].append(id2label[label_ids[batch_idx][seq_idx]])\n",
    "                pred_labels[batch_idx].append(id2label[preds[batch_idx][seq_idx]])\n",
    "                \n",
    "    return pred_labels, true_labels\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions, label_ids, _ = trainer.predict(tokenized_test_dataset)\n",
    "pred_labels, true_labels = align_predictions(predictions, label_ids)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Transformer NER Model Evaluation Results:\")\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "# Calculate metrics by entity type\n",
    "results_by_entity = {\n",
    "    'AUTHORITY': {'tp': 0, 'fp': 0, 'fn': 0},\n",
    "    'ADDRESSEE': {'tp': 0, 'fp': 0, 'fn': 0},\n",
    "    'SECTOR': {'tp': 0, 'fp': 0, 'fn': 0}\n",
    "}\n",
    "\n",
    "# Process predictions to extract entity-level metrics\n",
    "for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
    "    # Extract entities from the sequence\n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "    \n",
    "    # Extract true entities\n",
    "    current_entity = None\n",
    "    for i, label in enumerate(true_seq):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                true_entities.append(current_entity)\n",
    "            entity_type = label[2:]  # Remove \"B-\"\n",
    "            current_entity = {\"start\": i, \"end\": i+1, \"type\": entity_type}\n",
    "        elif label.startswith(\"I-\") and current_entity:\n",
    "            current_entity[\"end\"] = i+1\n",
    "        elif current_entity:\n",
    "            true_entities.append(current_entity)\n",
    "            current_entity = None\n",
    "    if current_entity:\n",
    "        true_entities.append(current_entity)\n",
    "    \n",
    "    # Extract predicted entities\n",
    "    current_entity = None\n",
    "    for i, label in enumerate(pred_seq):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                pred_entities.append(current_entity)\n",
    "            entity_type = label[2:]  # Remove \"B-\"\n",
    "            current_entity = {\"start\": i, \"end\": i+1, \"type\": entity_type}\n",
    "        elif label.startswith(\"I-\") and current_entity:\n",
    "            current_entity[\"end\"] = i+1\n",
    "        elif current_entity:\n",
    "            pred_entities.append(current_entity)\n",
    "            current_entity = None\n",
    "    if current_entity:\n",
    "        pred_entities.append(current_entity)\n",
    "    \n",
    "    # Count TP, FP, FN\n",
    "    for p_entity in pred_entities:\n",
    "        matched = False\n",
    "        for t_entity in true_entities:\n",
    "            if (p_entity[\"start\"] == t_entity[\"start\"] and \n",
    "                p_entity[\"end\"] == t_entity[\"end\"] and \n",
    "                p_entity[\"type\"] == t_entity[\"type\"]):\n",
    "                results_by_entity[p_entity[\"type\"]]['tp'] += 1\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            results_by_entity[p_entity[\"type\"]]['fp'] += 1\n",
    "    \n",
    "    for t_entity in true_entities:\n",
    "        if not any(p_entity[\"start\"] == t_entity[\"start\"] and \n",
    "                   p_entity[\"end\"] == t_entity[\"end\"] and \n",
    "                   p_entity[\"type\"] == t_entity[\"type\"] for p_entity in pred_entities):\n",
    "            results_by_entity[t_entity[\"type\"]]['fn'] += 1\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each entity type\n",
    "transformer_metrics = {}\n",
    "for entity_type, counts in results_by_entity.items():\n",
    "    tp = counts['tp']\n",
    "    fp = counts['fp']\n",
    "    fn = counts['fn']\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    transformer_metrics[entity_type] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "\n",
    "# Display metrics by entity type\n",
    "print(\"\\nTransformer NER Model Evaluation Results by Entity Type:\")\n",
    "for entity_type, metrics in transformer_metrics.items():\n",
    "    print(f\"\\nEntity Type: {entity_type}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"True Positives: {metrics['tp']}\")\n",
    "    print(f\"False Positives: {metrics['fp']}\")\n",
    "    print(f\"False Negatives: {metrics['fn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee3680",
   "metadata": {},
   "source": [
    "2.6 Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_policy_text_with_transformer(text):\n",
    "    \"\"\"Extract entities from text using the transformer-based NER model\"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = list(text)  # Simple character tokenization\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Process predictions\n",
    "    predictions = outputs.logits.argmax(dim=2).squeeze().tolist()\n",
    "    word_ids = inputs.word_ids()\n",
    "    \n",
    "    # Convert predictions to entities\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for idx, pred_id in enumerate(predictions):\n",
    "        if word_ids[idx] is None:\n",
    "            continue\n",
    "            \n",
    "        token = tokens[word_ids[idx]]\n",
    "        pred_label = id2label.get(pred_id, \"O\")\n",
    "        \n",
    "        if pred_label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            entity_type = pred_label[2:]\n",
    "            current_entity = {\"start\": word_ids[idx], \"text\": token, \"type\": entity_type}\n",
    "        elif pred_label.startswith(\"I-\") and current_entity:\n",
    "            current_entity[\"text\"] += token\n",
    "        elif current_entity:\n",
    "            entities.append(current_entity)\n",
    "            current_entity = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "        \n",
    "    return {\n",
    "        'text': text,\n",
    "        'entities': [\n",
    "            {\n",
    "                'text': entity['text'],\n",
    "                'label': entity['type'],\n",
    "                'start': entity['start'],\n",
    "                'end': entity['start'] + len(entity['text'])\n",
    "            } for entity in entities\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Test the inference pipeline on a sample\n",
    "sample_text = test_df['context'].iloc[0]\n",
    "results = analyze_policy_text_with_transformer(sample_text)\n",
    "\n",
    "print(\"Sample inference results:\")\n",
    "print(f\"Text: '{sample_text[:100]}...'\")\n",
    "print(\"\\nEntities found:\")\n",
    "for entity in results['entities']:\n",
    "    print(f\"  - '{entity['text']}' ({entity['label']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3b338",
   "metadata": {},
   "source": [
    "2.7 Comparison with spaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your previously trained spaCy NER model\n",
    "import spacy\n",
    "try:\n",
    "    spacy_ner = spacy.load(\"./ner_data/model/model-best\")\n",
    "    print(\"Successfully loaded trained spaCy NER model\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading spaCy model: {e}\")\n",
    "    print(\"Using base spaCy model as a fallback\")\n",
    "    spacy_ner = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Add custom entity types\n",
    "    if 'ner' not in spacy_ner.pipe_names:\n",
    "        ner = spacy_ner.add_pipe('ner')\n",
    "    else:\n",
    "        ner = spacy_ner.get_pipe('ner')\n",
    "        \n",
    "    # Add custom entity labels\n",
    "    for label in ['AUTHORITY', 'ADDRESSEE', 'SECTOR']:\n",
    "        if label not in ner.labels:\n",
    "            ner.add_label(label)\n",
    "\n",
    "# Define a function to run and compare both models on the same text\n",
    "def compare_ner_models(text):\n",
    "    \"\"\"Compare spaCy and transformer NER models on the same text\"\"\"\n",
    "    # Process with spaCy NER\n",
    "    spacy_doc = spacy_ner(text)\n",
    "    spacy_entities = [\n",
    "        {\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'start': ent.start_char,\n",
    "            'end': ent.end_char\n",
    "        } for ent in spacy_doc.ents\n",
    "    ]\n",
    "    \n",
    "    # Process with transformer NER\n",
    "    transformer_results = analyze_policy_text_with_transformer(text)\n",
    "    transformer_entities = transformer_results['entities']\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'spacy_entities': spacy_entities,\n",
    "        'transformer_entities': transformer_entities\n",
    "    }\n",
    "\n",
    "# Run comparison on a few test examples\n",
    "comparison_results = []\n",
    "for i in range(min(3, len(test_df))):\n",
    "    text = test_df['context'].iloc[i]\n",
    "    result = compare_ner_models(text)\n",
    "    comparison_results.append(result)\n",
    "\n",
    "# Display comparison results\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "for i, result in enumerate(comparison_results):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Text: '{result['text'][:100]}...'\")\n",
    "    \n",
    "    print(\"\\nspaCy NER entities:\")\n",
    "    for entity in result['spacy_entities']:\n",
    "        print(f\"  - '{entity['text']}' ({entity['label']})\")\n",
    "    \n",
    "    print(\"\\nTransformer NER entities:\")\n",
    "    for entity in result['transformer_entities']:\n",
    "        print(f\"  - '{entity['text']}' ({entity['label']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7913ecf",
   "metadata": {},
   "source": [
    "2.8 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance metrics of both models\n",
    "def plot_comparison(spacy_metrics, transformer_metrics):\n",
    "    \"\"\"Create comparative visualizations of model performance\"\"\"\n",
    "    # Prepare data for plotting\n",
    "    entities = list(spacy_metrics.keys())\n",
    "    metrics = ['precision', 'recall', 'f1']\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        # Extract metric values for each entity type\n",
    "        spacy_values = [spacy_metrics[entity][metric] for entity in entities]\n",
    "        transformer_values = [transformer_metrics[entity][metric] for entity in entities]\n",
    "        \n",
    "        # Set up bar positions\n",
    "        x = np.arange(len(entities))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Create bars\n",
    "        axes[i].bar(x - width/2, spacy_values, width, label='spaCy NER')\n",
    "        axes[i].bar(x + width/2, transformer_values, width, label='Transformer NER')\n",
    "        \n",
    "        # Add labels and styling\n",
    "        axes[i].set_xlabel('Entity Type')\n",
    "        axes[i].set_ylabel(metric.capitalize())\n",
    "        axes[i].set_title(f'{metric.capitalize()} Comparison')\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(entities)\n",
    "        axes[i].grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, v in enumerate(spacy_values):\n",
    "            axes[i].text(j - width/2, v + 0.02, f'{v:.2f}', ha='center')\n",
    "        for j, v in enumerate(transformer_values):\n",
    "            axes[i].text(j + width/2, v + 0.02, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    axes[0].legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Extract spaCy metrics from your previous execution\n",
    "try:\n",
    "    # Load previously calculated spaCy metrics\n",
    "    # Note: In a real implementation, you would want to reference your actual metrics\n",
    "    spacy_metrics = {\n",
    "        'AUTHORITY': {\n",
    "            'precision': 0.5608,\n",
    "            'recall': 0.6331,\n",
    "            'f1': 0.5947,\n",
    "            'tp': 226,\n",
    "            'fp': 177,\n",
    "            'fn': 131\n",
    "        },\n",
    "        'ADDRESSEE': {\n",
    "            'precision': 0.5913,\n",
    "            'recall': 0.6044,\n",
    "            'f1': 0.5978,\n",
    "            'tp': 327,\n",
    "            'fp': 226,\n",
    "            'fn': 214\n",
    "        },\n",
    "        'SECTOR': {\n",
    "            'precision': 0.3802,\n",
    "            'recall': 0.5387,\n",
    "            'f1': 0.4458,\n",
    "            'tp': 146,\n",
    "            'fp': 238,\n",
    "            'fn': 125\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Compare model performance\n",
    "    plot_comparison(spacy_metrics, transformer_metrics)\n",
    "    \n",
    "    # Print overall comparison summary\n",
    "    print(\"\\nModel Comparison Summary:\")\n",
    "    print(\"Average metrics across all entity types:\")\n",
    "    \n",
    "    spacy_avg = {\n",
    "        'precision': sum(m['precision'] for m in spacy_metrics.values()) / len(spacy_metrics),\n",
    "        'recall': sum(m['recall'] for m in spacy_metrics.values()) / len(spacy_metrics),\n",
    "        'f1': sum(m['f1'] for m in spacy_metrics.values()) / len(spacy_metrics),\n",
    "    }\n",
    "    \n",
    "    transformer_avg = {\n",
    "        'precision': sum(m['precision'] for m in transformer_metrics.values()) / len(transformer_metrics),\n",
    "        'recall': sum(m['recall'] for m in transformer_metrics.values()) / len(transformer_metrics),\n",
    "        'f1': sum(m['f1'] for m in transformer_metrics.values()) / len(transformer_metrics),\n",
    "    }\n",
    "    \n",
    "    print(f\"spaCy NER - Avg Precision: {spacy_avg['precision']:.4f}, Avg Recall: {spacy_avg['recall']:.4f}, Avg F1: {spacy_avg['f1']:.4f}\")\n",
    "    print(f\"Transformer NER - Avg Precision: {transformer_avg['precision']:.4f}, Avg Recall: {transformer_avg['recall']:.4f}, Avg F1: {transformer_avg['f1']:.4f}\")\n",
    "    \n",
    "    # Improvement percentage\n",
    "    f1_improvement = (transformer_avg['f1'] - spacy_avg['f1']) / spacy_avg['f1'] * 100\n",
    "    print(f\"\\nF1 Score improvement: {f1_improvement:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in performance comparison: {e}\")\n",
    "    print(\"Please ensure you have both spaCy and transformer metrics available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
