{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "title-section",
      "metadata": {},
      "source": [
       "# Comprehensive Annotation Comparison\n",
       "This notebook compares annotations from:\n",
       "1. Individual coders against curated annotations (\"gold standard\")\n",
       "2. LLM-based annotations against curated annotations\n",
       "\n",
       "The goal is to understand how individual human coders compare to the curated gold standard, and then see how the LLM-based methods perform in relation to both individual coders and the curated annotations."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "imports-section",
      "metadata": {},
      "source": [
       "## 1. Import Required Modules"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import sys\n",
       "import json\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "from pathlib import Path\n",
       "from typing import Dict, List, Optional, Any, Union, Tuple\n",
       "\n",
       "# Add parent directory to path to import our modules\n",
       "sys.path.append('..')\n",
       "\n",
       "# Import our custom modules\n",
       "from src.utils import (\n",
       "    get_project_root, \n",
       "    load_coding_scheme, \n",
       "    filter_coding_scheme,\n",
       "    load_raw_text,\n",
       "    load_curated_annotations,\n",
       "    load_coder_annotations,\n",
       "    get_articles_paths\n",
       ")\n",
       "\n",
       "from src.evaluation import (\n",
       "    evaluate_article,\n",
       "    compare_coder_to_curated,\n",
       "    get_absolute_metrics\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "setup-section",
      "metadata": {},
      "source": [
       "## 2. Configure Evaluation Parameters"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Article to evaluate\n",
       "article_id = \"EU_32018R1999_Title_0_Chapter_6_Section_3_Article_37\"\n",
       "\n",
       "# Target layer and tagset\n",
       "target_layer = \"Policydesigncharacteristics\"\n",
       "target_tagset = \"Actor\"\n",
       "\n",
       "# Boundary tolerance in characters\n",
       "tolerance = 15\n",
       "\n",
       "# Paths to standard and extended model annotations\n",
       "root_dir = get_project_root()\n",
       "article_dir = os.path.join(root_dir, 'data', '03b_processed_to_json', article_id)\n",
       "standard_path = os.path.join(article_dir, 'Generated_Annotations_Standard.json')\n",
       "extended_path = os.path.join(article_dir, 'Generated_Annotations_Extended.json')\n",
       "\n",
       "# Verify the files exist\n",
       "if not os.path.exists(standard_path):\n",
       "    print(f\"Warning: Standard annotations file not found at {standard_path}\")\n",
       "if not os.path.exists(extended_path):\n",
       "    print(f\"Warning: Extended annotations file not found at {extended_path}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "helper-functions-section",
      "metadata": {},
      "source": [
       "## 3. Helper Functions"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "helper-functions",
      "metadata": {},
      "outputs": [],
      "source": [
       "def get_coders_for_article(article_id: str) -> List[str]:\n",
       "    \"\"\"Get list of coder IDs that have annotated a specific article.\"\"\"\n",
       "    try:\n",
       "        coder_annotations = load_coder_annotations(article_id)\n",
       "        return list(coder_annotations.keys())\n",
       "    except FileNotFoundError:\n",
       "        return []\n",
       "    except Exception as e:\n",
       "        print(f\"Error loading coder annotations for {article_id}: {e}\")\n",
       "        return []\n",
       "\n",
       "def create_comparison_dataframe(article_id: str, layers: List[str], tagsets: List[str], tolerance: int = 15) -> pd.DataFrame:\n",
       "    \"\"\"Create a DataFrame comparing individual coders and LLM annotations against curated annotations.\"\"\"\n",
       "    # Dictionary to store results for each annotator\n",
       "    results = {}\n",
       "    \n",
       "    # Get coder IDs\n",
       "    coders = get_coders_for_article(article_id)\n",
       "    \n",
       "    # Compare each coder to curated annotations\n",
       "    for coder_id in coders:\n",
       "        try:\n",
       "            coder_results = compare_coder_to_curated(\n",
       "                article_id=article_id,\n",
       "                coder_id=coder_id,\n",
       "                layers=layers,\n",
       "                tagsets=tagsets,\n",
       "                tolerance=tolerance\n",
       "            )\n",
       "            results[coder_id] = coder_results\n",
       "        except Exception as e:\n",
       "            print(f\"Error comparing coder {coder_id} to curated annotations: {e}\")\n",
       "    \n",
       "    # Compare LLM standard annotations to curated\n",
       "    try:\n",
       "        standard_path = os.path.join(get_project_root(), 'data', '03b_processed_to_json', \n",
       "                                   article_id, 'Generated_Annotations_Standard.json')\n",
       "        if os.path.exists(standard_path):\n",
       "            standard_results = evaluate_article(\n",
       "                article_id=article_id,\n",
       "                generated_path=standard_path,\n",
       "                layers=layers,\n",
       "                tagsets=tagsets,\n",
       "                save_results=False,\n",
       "                tolerance=tolerance\n",
       "            )\n",
       "            results['Standard'] = standard_results\n",
       "    except Exception as e:\n",
       "        print(f\"Error evaluating standard annotations: {e}\")\n",
       "    \n",
       "    # Compare LLM extended annotations to curated\n",
       "    try:\n",
       "        extended_path = os.path.join(get_project_root(), 'data', '03b_processed_to_json', \n",
       "                                    article_id, 'Generated_Annotations_Extended.json')\n",
       "        if os.path.exists(extended_path):\n",
       "            extended_results = evaluate_article(\n",
       "                article_id=article_id,\n",
       "                generated_path=extended_path,\n",
       "                layers=layers,\n",
       "                tagsets=tagsets,\n",
       "                save_results=False,\n",
       "                tolerance=tolerance\n",
       "            )\n",
       "            results['Extended'] = extended_results\n",
       "    except Exception as e:\n",
       "        print(f\"Error evaluating extended annotations: {e}\")\n",
       "    \n",
       "    # Create a DataFrame from the results\n",
       "    data = []\n",
       "    for annotator, result in results.items():\n",
       "        # Get absolute metrics\n",
       "        abs_metrics = get_absolute_metrics(result)\n",
       "        \n",
       "        data.append({\n",
       "            'Annotator': annotator,\n",
       "            'Span F1': result['summary']['span_f1'],\n",
       "            'Tag Accuracy': result['tag_assignment']['tag_accuracy'],\n",
       "            'Full Match Acc': result['tag_assignment']['full_match_accuracy'],\n",
       "            'Combined': result['summary']['combined_score'],\n",
       "            'Correct Anno': abs_metrics['correct_annotations'],\n",
       "            'Extra Anno': abs_metrics['extra_annotations'],\n",
       "            'Missed Anno': abs_metrics['missed_annotations'],\n",
       "            'Total Anno': abs_metrics.get('total_generated', 0),\n",
       "            'Total Curated': abs_metrics.get('total_curated', 0)\n",
       "        })\n",
       "    \n",
       "    return pd.DataFrame(data)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "single-article-section",
      "metadata": {},
      "source": [
       "## 4. Single Article Evaluation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "single-article-eval",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Check if the article has individual coder annotations\n",
       "coders = get_coders_for_article(article_id)\n",
       "print(f\"Found {len(coders)} coders for article {article_id}: {', '.join(coders)}\")\n",
       "\n",
       "# Create the comparison DataFrame\n",
       "comparison_df = create_comparison_dataframe(\n",
       "    article_id=article_id,\n",
       "    layers=[target_layer],\n",
       "    tagsets=[target_tagset],\n",
       "    tolerance=tolerance\n",
       ")\n",
       "\n",
       "# Display the results\n",
       "print(f\"\\nComparison for article {article_id}, layer {target_layer}, tagset {target_tagset} (tolerance: Â±{tolerance} characters):\\n\")\n",
       "comparison_df.set_index('Annotator', inplace=True)\n",
       "display(comparison_df)\n",
       "\n",
       "# Format the table for better readability\n",
       "styled_df = comparison_df.style.format({\n",
       "    'Span F1': '{:.4f}',\n",
       "    'Tag Accuracy': '{:.4f}',\n",
       "    'Full Match Acc': '{:.4f}',\n",
       "    'Combined': '{:.4f}',\n",
       "    'Correct Anno': '{:.0f}',\n",
       "    'Extra Anno': '{:.0f}',\n",
       "    'Missed Anno': '{:.0f}',\n",
       "    'Total Anno': '{:.0f}',\n",
       "    'Total Curated': '{:.0f}'\n",
       "})\n",
       "\n",
       "# Display the styled table\n",
       "display(styled_df)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "visualization-section",
      "metadata": {},
      "source": [
       "## 5. Visualize the Results"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualize-results",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create bar charts for key metrics\n",
       "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
       "metrics = ['Span F1', 'Full Match Acc', 'Correct Anno', 'Extra Anno']\n",
       "titles = ['Span F1 Score', 'Full Match Accuracy', 'Correctly Annotated Spans', 'Extra Annotations']\n",
       "colors = ['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4']\n",
       "\n",
       "# Add highlighting for LLM methods\n",
       "if 'Standard' in comparison_df.index:\n",
       "    colors[comparison_df.index.get_loc('Standard')] = '#ff7f0e'\n",
       "if 'Extended' in comparison_df.index:\n",
       "    colors[comparison_df.index.get_loc('Extended')] = '#2ca02c'\n",
       "\n",
       "# Create each subplot\n",
       "for i, (ax, metric, title) in enumerate(zip(axes.flatten(), metrics, titles)):\n",
       "    comparison_df[metric].plot(kind='bar', ax=ax, color=colors)\n",
       "    ax.set_title(title)\n",
       "    ax.set_ylabel(metric)\n",
       "    \n",
       "    # Add value labels on top of bars\n",
       "    for j, v in enumerate(comparison_df[metric]):\n",
       "        if metric in ['Span F1', 'Full Match Acc']:\n",
       "            ax.text(j, v + 0.01, f\"{v:.4f}\", ha='center')\n",
       "        else:\n",
       "            ax.text(j, v + 0.5, f\"{int(v)}\", ha='center')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "multi-article-section",
      "metadata": {},
      "source": [
       "## 6. Multi-Article Evaluation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "find-articles",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Get a list of all articles\n",
       "all_articles = get_articles_paths()\n",
       "print(f\"Found {len(all_articles)} articles in total\")\n",
       "\n",
       "# Find articles that have both coder annotations and LLM annotations\n",
       "valid_articles = []\n",
       "for article in all_articles:\n",
       "    article_id = article['id']\n",
       "    coders = get_coders_for_article(article_id)\n",
       "    \n",
       "    # Check if standard and extended annotations exist\n",
       "    standard_path = os.path.join(root_dir, 'data', '03b_processed_to_json', \n",
       "                               article_id, 'Generated_Annotations_Standard.json')\n",
       "    extended_path = os.path.join(root_dir, 'data', '03b_processed_to_json', \n",
       "                                article_id, 'Generated_Annotations_Extended.json')\n",
       "    \n",
       "    if len(coders) > 0 and os.path.exists(standard_path) and os.path.exists(extended_path):\n",
       "        valid_articles.append({\n",
       "            'id': article_id,\n",
       "            'coders': coders\n",
       "        })\n",
       "\n",
       "print(f\"Found {len(valid_articles)} articles with both coder and LLM annotations\")\n",
       "print(f\"First 5 valid articles: {[a['id'] for a in valid_articles[:5]]}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "multi-article-eval",
      "metadata": {},
      "outputs": [],
      "source": [
       "# If there are too many articles, limit to a smaller sample\n",
       "sample_size = min(5, len(valid_articles))\n",
       "sample_articles = valid_articles[:sample_size]\n",
       "\n",
       "# Create a DataFrame for each article and then concatenate them\n",
       "all_comparisons = []\n",
       "\n",
       "for article in sample_articles:\n",
       "    article_id = article['id']\n",
       "    try:\n",
       "        # Create comparison DataFrame for this article\n",
       "        df = create_comparison_dataframe(\n",
       "            article_id=article_id,\n",
       "            layers=[target_layer],\n",
       "            tagsets=[target_tagset],\n",
       "            tolerance=tolerance\n",
       "        )\n",
       "        df['Article'] = article_id  # Add article ID as a column\n",
       "        all_comparisons.append(df)\n",
       "    except Exception as e:\n",
       "        print(f\"Error processing article {article_id}: {e}\")\n",
       "\n",
       "# Combine all comparisons into a single DataFrame\n",
       "if all_comparisons:\n",
       "    combined_df = pd.concat(all_comparisons, ignore_index=True)\n",
       "    print(f\"\\nCombined results across {len(all_comparisons)} articles:\")\n",
       "    display(combined_df.head())\n",
       "else:\n",
       "    print(\"No comparison data available for the selected articles.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "multi-article-aggregation",
      "metadata": {},
      "outputs": [],
      "source": [
       "if all_comparisons:\n",
       "    # Calculate average performance by annotator type\n",
       "    aggregated_df = combined_df.groupby('Annotator').agg({\n",
       "        'Span F1': 'mean',\n",
       "        'Tag Accuracy': 'mean',\n",
       "        'Full Match Acc': 'mean',\n",
       "        'Combined': 'mean',\n",
       "        'Correct Anno': 'sum',\n",
       "        'Extra Anno': 'sum',\n",
       "        'Missed Anno': 'sum',\n",
       "        'Total Anno': 'sum',\n",
       "        'Total Curated': 'mean'\n",
       "    })\n",
       "    \n",
       "    print(\"\\nAverage performance by annotator type:\")\n",
       "    # Format the aggregated table for better readability\n",
       "    styled_agg_df = aggregated_df.style.format({\n",
       "        'Span F1': '{:.4f}',\n",
       "        'Tag Accuracy': '{:.4f}',\n",
       "        'Full Match Acc': '{:.4f}',\n",
       "        'Combined': '{:.4f}',\n",
       "        'Correct Anno': '{:.0f}',\n",
       "        'Extra Anno': '{:.0f}',\n",
       "        'Missed Anno': '{:.0f}',\n",
       "        'Total Anno': '{:.0f}',\n",
       "        'Total Curated': '{:.1f}'\n",
       "    })\n",
       "    \n",
       "    display(styled_agg_df)\n",
       "    \n",
       "    # Visualize the aggregated results\n",
       "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
       "    metrics = ['Span F1', 'Full Match Acc', 'Correct Anno', 'Extra Anno']\n",
       "    titles = ['Average Span F1 Score', 'Average Full Match Accuracy', \n",
       "              'Total Correctly Annotated Spans', 'Total Extra Annotations']\n",
       "    \n",
       "    # Create a color map\n",
       "    coder_ids = [col for col in aggregated_df.index if col not in ['Standard', 'Extended']]\n",
       "    color_map = {coder_id: '#1f77b4' for coder_id in coder_ids}\n",
       "    if 'Standard' in aggregated_df.index:\n",
       "        color_map['Standard'] = '#ff7f0e'\n",
       "    if 'Extended' in aggregated_df.index:\n",
       "        color_map['Extended'] = '#2ca02c'\n",
       "        \n",
       "    colors = [color_map.get(idx, '#1f77b4') for idx in aggregated_df.index]\n",
       "    \n",
       "    # Create each subplot\n",
       "    for i, (ax, metric, title) in enumerate(zip(axes.flatten(), metrics, titles)):\n",
       "        aggregated_df[metric].plot(kind='bar', ax=ax, color=colors)\n",
       "        ax.set_title(title)\n",
       "        ax.set_ylabel(metric)\n",
       "        \n",
       "        # Add value labels on top of bars\n",
       "        for j, v in enumerate(aggregated_df[metric]):\n",
       "            if metric in ['Span F1', 'Full Match Acc']:\n",
       "                ax.text(j, v + 0.01, f\"{v:.4f}\", ha='center')\n",
       "            else:\n",
       "                ax.text(j, v + 0.5, f\"{int(v)}\", ha='center')\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "    \n",
       "    # Create a table comparing human coders vs LLM methods\n",
       "    coder_avg = combined_df[combined_df['Annotator'].isin(coder_ids)].mean(numeric_only=True)\n",
       "    standard_avg = combined_df[combined_df['Annotator'] == 'Standard'].mean(numeric_only=True) if 'Standard' in combined_df['Annotator'].values else None\n",
       "    extended_avg = combined_df[combined_df['Annotator'] == 'Extended'].mean(numeric_only=True) if 'Extended' in combined_df['Annotator'].values else None\n",
       "    \n",
       "    comparison_data = {'Human Coders': coder_avg}\n",
       "    if standard_avg is not None:\n",
       "        comparison_data['Standard LLM'] = standard_avg\n",
       "    if extended_avg is not None:\n",
       "        comparison_data['Extended LLM'] = extended_avg\n",
       "        \n",
       "    comparison_summary = pd.DataFrame(comparison_data).T[['Span F1', 'Full Match Acc', 'Correct Anno', 'Extra Anno']]\n",
       "    print(\"\\nComparison summary (human coders vs LLM methods):\")\n",
       "    display(comparison_summary.style.format({\n",
       "        'Span F1': '{:.4f}',\n",
       "        'Full Match Acc': '{:.4f}',\n",
       "        'Correct Anno': '{:.1f}',\n",
       "        'Extra Anno': '{:.1f}'\n",
       "    }))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "conclusion-section",
      "metadata": {},
      "source": [
       "## 7. Conclusion\n",
       "\n",
       "This notebook provides a comprehensive comparison between:\n",
       "1. Individual human coder annotations and the curated \"gold standard\" annotations\n",
       "2. LLM-based annotations (standard and extended schemes) and the curated annotations\n",
       "\n",
       "Key findings:\n",
       "* [Add your observations here based on the results]\n",
       "* [Note any patterns in how individual coders compare to the gold standard]\n",
       "* [Compare LLM performance to individual coders]\n",
       "* [Discuss the impact of using the extended scheme with examples]\n",
       "\n",
       "The results suggest that [add your conclusions here].\n",
       "\n",
       "**Future work:**\n",
       "- Run this analysis across a larger set of articles to get more robust findings\n",
       "- Explore additional layers and tagsets beyond Actor\n",
       "- Investigate factors that contribute to differences between individual coders and the curated standard"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "POLIANNA",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }